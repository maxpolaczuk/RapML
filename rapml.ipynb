{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RapML \n",
    "\n",
    "Deep learning for rap lyrics generation using Seq2Seq Learning. Inspired by machine translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, RepeatVector, TimeDistributed, LeakyReLU, BatchNormalization\n",
    "import random\n",
    "from rapml_functions import *\n",
    "import math\n",
    "\n",
    "# read in the vocab\n",
    "vocab = pd.read_csv('rap_vocab.csv')\n",
    "\n",
    "# get size of vocab\n",
    "vocabsize = np.max(vocab.ix[:,0].tolist())\n",
    "print(vocabsize) \n",
    "del vocab\n",
    "\n",
    "print('vocab size is' , vocabsize)\n",
    "#=============================================================================\n",
    "# make functions to prepare data\n",
    "\n",
    "def zero_pad(row, hard_size = 20):\n",
    "    # pass in a pandas row \n",
    "    try:\n",
    "        row = row.tolist()\n",
    "        size = len(row) # this is the number of dimensions of the sequence\n",
    "    except:\n",
    "        #print('using list type already')\n",
    "        size = hard_size     \n",
    "\n",
    "    # make a sparse array the size of vocab & sequence:\n",
    "    tmp = np.zeros(size, dtype=int)\n",
    "\n",
    "    # need to pad out the number of entries\n",
    "    num_entries = len([x for x in row if str(x) != 'nan']) # how many in current sequence\n",
    "\n",
    "    # make the non-zero row entries reversed:\n",
    "    row = ([x for x in row if str(x) != 'nan'])[::-1]\n",
    "    \n",
    "    # pad it to length we require\n",
    "    padding = size - num_entries\n",
    "\n",
    "    for i in range(num_entries):\n",
    "        # one hot this\n",
    "        tmp[i+padding] = int(row[i])\n",
    "\n",
    "    return tmp.tolist()    \n",
    "\n",
    "\n",
    "def embed_and_pad(row, vocab_size = vocabsize):\n",
    "    # this is for outputs...\n",
    "    # pass in a pandas row \n",
    "    try:\n",
    "        row = row.tolist()\n",
    "    except:\n",
    "        #print('using list type already')\n",
    "        pass\n",
    "\n",
    "    size = len(row) # this is the number of dimensions of the sequence\n",
    "\n",
    "    # make a sparse array the size of vocab & sequence:\n",
    "    tmp = np.zeros((size, vocab_size+1))\n",
    "\n",
    "    # need to pad out the number of entries\n",
    "    num_entries = len([x for x in row if str(x) != 'nan']) # how many in current sequence\n",
    "\n",
    "    # pad it to length we require\n",
    "    padding = size - num_entries\n",
    "\n",
    "    for i in range(num_entries):\n",
    "        # push the sequence to the left...\n",
    "        # one hot this vector\n",
    "        tmp[i,int(row[i])] = 1\n",
    "    return tmp    \n",
    "\n",
    "\n",
    "def get_sampleweight(row):\n",
    "    # this is for sample weights of output...\n",
    "    # pass in a pandas row \n",
    "    try:\n",
    "        row = row.tolist()\n",
    "    except:\n",
    "        #print('using list type already')\n",
    "\n",
    "    size = len(row) # this is the number of dimensions of the sequence\n",
    "\n",
    "    # make an array the size of samples & sequence:\n",
    "    tmp = np.zeros(size)\n",
    "\n",
    "    # need to pad out the number of entries\n",
    "    num_entries = len([x for x in row if str(x) != 'nan']) # how many in current sequence\n",
    "\n",
    "    # pad it to length we require\n",
    "    padding = size - num_entries\n",
    "\n",
    "    for i in range(num_entries):\n",
    "        # one hot this\n",
    "        tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "def get_word_index(word, LUT):\n",
    "    ''' \n",
    "    takes in the word, finds it location in the LUT or assigns\n",
    "    the UNK index number to it \n",
    "    '''\n",
    "    try:\n",
    "        # we have the word in vocab\n",
    "        indx = LUT[str(word)]\n",
    "    except:\n",
    "        # it's not in vocabulary\n",
    "        indx = LUT['UNK']\n",
    "\n",
    "    return indx\n",
    "\n",
    "def thresh(x,thres):\n",
    "    if x > thres:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def decode_prediction(prediction, temp = 0.1):\n",
    "    # takes in one line of prediction 2D array\n",
    "    # (num_seq, vocab):\n",
    "\n",
    "    # for each sequence get maximum word activation:\n",
    "    phrase = []\n",
    "    conf = []\n",
    "\n",
    "    for word in range(np.shape(prediction)[0]):\n",
    "        # change prediction to have probability dist of a certain temp:\n",
    "        ogpreds = prediction[word] # archive this\n",
    "        prediction[word] = [thresh(x, temp) for x in prediction[word]] # rescale\n",
    "        prediction[word] = [x/np.sum(prediction[word]) for x in prediction[word]]\n",
    "        #idx = np.argmax(prediction[word]) # to get the max likelihood\n",
    "        if True:#try:\n",
    "            # choose 1 number from the sampled probabilities\n",
    "            idx = np.random.choice(range(len(prediction[word])),1,p=prediction[word])[0]\n",
    "            while (idx == 8378)  &  (np.count_nonzero(prediction[word]) > 1) :\n",
    "                # resample from the distribution - so to make sure it's not zambia!:\n",
    "                idx = np.random.choice(range(len(prediction[word])),1,p=prediction[word])[0]\n",
    "\n",
    "            if math.isnan(prediction[word][idx]):\n",
    "                continue\n",
    "            else:\n",
    "                # also now remove if duplicate:\n",
    "                if word > 0:\n",
    "                    while (str(phrase[word-1]) == str(idx))  & (np.count_nonzero(prediction[word]) > 1):\n",
    "                        # resample from the distribution - so to make sure it's not a duplicate!:\n",
    "                        idx = np.random.choice(range(len(prediction[word])),1,p=prediction[word])[0]  \n",
    "                        \n",
    "                    phrase.append(idx) # add in the confidence score and the word index\n",
    "                    conf.append(ogpreds[idx]) \n",
    "                        \n",
    "                else:\n",
    "                    phrase.append(idx) # add in the confidence score and the word index\n",
    "                    conf.append(ogpreds[idx])\n",
    "        #except:\n",
    "        #    print('didnt work')\n",
    "    return phrase, conf\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "## because of RAM limitations, make the data into batches: \n",
    "\n",
    "def make_batch(inputs_ = pd.read_csv('inputs.csv'),targets_ = pd.read_csv('targets.csv'),batch_size = 1000):\n",
    "    # they should have same number of samples\n",
    "\n",
    "    # randomly assign 2000 index numbers to make into a batch\n",
    "    idxs = random.sample(range(len(inputs_)), batch_size)\n",
    "\n",
    "    # convert the data into their sampled form...\n",
    "\n",
    "    ins = [] # empty list\n",
    "    outs = [] \n",
    "    sampleweight = []\n",
    "    for i in idxs:\n",
    "        # convert the nans to none\n",
    "        ins.append(zero_pad(inputs_.ix[i,:]))\n",
    "        outs.append(embed_and_pad(targets_.ix[i,:]))\n",
    "        sampleweight.append(get_sampleweight(targets_.ix[i,:]))\n",
    "\n",
    "    del idxs\n",
    "\n",
    "    return np.array(ins), np.array(outs), np.array(sampleweight) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "# make the data using the above function\n",
    "ins, targs, smpweight = make_batch()\n",
    "\n",
    "# build model architecture in keras - (Seq2Seq)\n",
    "# easy and straightforward so can just use sequential model\n",
    "if True == True: \n",
    "    # hyperparams\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 128\n",
    "    embed_dim = 128\n",
    "    densedim = 2048\n",
    "    n_nxt = 20 # dimension of output sequences\n",
    "\n",
    "    # Seq2Seq model structure\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabsize+1,embed_dim, input_length=n_nxt, mask_zero = True))\n",
    "\n",
    "    # batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "    # encoder network\n",
    "    model.add(LSTM(hidden_dim1, return_sequences = True, stateful= False,forget_bias_init='one'))\n",
    "    model.add(LSTM(hidden_dim2, return_sequences = True, stateful= False,forget_bias_init='one'))\n",
    "    model.add(LSTM(hidden_dim2, return_sequences = False, stateful= False,forget_bias_init='one'))\n",
    "    model.add(Dropout(0.2)) # no dropout\n",
    "    model.add(Dense(densedim, activation='linear'))\n",
    "    # batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(RepeatVector(n_nxt))\n",
    "\n",
    "    # decoder network\n",
    "    model.add(LSTM(hidden_dim2,return_sequences=True,stateful=False,forget_bias_init='one'))       \n",
    "    model.add(LSTM(hidden_dim2,return_sequences=True,stateful=False,forget_bias_init='one'))\n",
    "    model.add(LSTM(hidden_dim2,return_sequences=True,stateful=False,forget_bias_init='one'))\n",
    "    model.add(TimeDistributed(Dense(vocabsize+1, activation = \"softmax\")))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',sample_weight_mode='temporal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "# load in model from last saved file:\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('rapML model9.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL\n",
    "\n",
    "Run this cell as many times as you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del ins\n",
    "    del targs\n",
    "except:\n",
    "    print('those inputs / outputs dont exist yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### make the data using the above function\n",
    "ins, targs, smpweight = make_batch()\n",
    "\n",
    "# fit the model in batches:\n",
    "for i in range(100000):\n",
    "#for i in range(epochs):\n",
    "    print('BATCH %s' % i)\n",
    "    if i > 0:\n",
    "        # not the first batch so make some new data\n",
    "        try:\n",
    "            # remove the old input data - so we have memory:\n",
    "            del ins\n",
    "            del targs\n",
    "            del smpweight\n",
    "        except:\n",
    "            print('ins and targs are already deleted')\n",
    "        # make some new data:\n",
    "        ins, targs, smpweight = make_batch()\n",
    "\n",
    "    # fit the model on this batch\n",
    "    model.fit(ins,targs, validation_split = 0.1, batch_size=200,nb_epoch = 1, sample_weight= smpweight)\n",
    "    # remove ins and targs for memory:\n",
    "    del ins\n",
    "    del targs\n",
    "    del smpweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT\n",
    "\n",
    "Interactive session for predicting following lines from what you write in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKE A PREDICTION!\n",
      "Please enter your first line: \n",
      "It don't matter, he's dope\n",
      "predicting next verse from:  it don't matter, he's dope\n",
      "using list type already\n",
      "prediction is:\n",
      " they don't be kill a fake yeah get high yeah it it it kickin' kickin' little kickin' little kickin' wet \n",
      "do another phrase? (y/n): n\n"
     ]
    }
   ],
   "source": [
    "# make a prediction:\n",
    "print('MAKE A PREDICTION!')\n",
    "# read back in vocabulary\n",
    "vocab = pd.read_csv('rap_vocab.csv')\n",
    "# make vocab lookup table\n",
    "LUT = {  str(vocab.ix[i,1]) : vocab.ix[i,0] for i in range(len(vocab))} \n",
    "LUT['UNK'] = len(LUT) + 1 # add the UNKnown words at the end of dictionary\n",
    "LUT['UNK'] = len(LUT) + 2 # add the UNKnown words at the end of dictionary\n",
    "\n",
    "# make reverse lookup:\n",
    "INVLUT = { vocab.ix[i,0] :  str(vocab.ix[i,1]) for i in range(len(vocab))} \n",
    "INVLUT[len(LUT)] = 'UNK'\n",
    "INVLUT[len(LUT)+1] = 'UNK'\n",
    "\n",
    "del vocab # for memory\n",
    "\n",
    "\n",
    "close = False # \n",
    "while close == False:\n",
    "    predline = input('Please enter your first line: \\n')\n",
    "    print('predicting next verse from: ', rap_izer(predline))\n",
    "    # \n",
    "    #print('splitting into common words:')\n",
    "    predline_inp = rap_izer(predline).split()\n",
    "\n",
    "    pred_inp = [] # setup empty array\n",
    "    # get each index from vocab:\n",
    "    for word in predline_inp:\n",
    "        pred_inp.append(get_word_index(word,LUT))\n",
    "\n",
    "    # convert pred_inp into np array:\n",
    "    pred_inp = np.array(zero_pad(pred_inp))\n",
    "    #print('shape of array is: ', np.shape(pred_inp))\n",
    "    # reshape maybe:\n",
    "    pred_inp = np.reshape(pred_inp,(1,np.shape(pred_inp)[0]))\n",
    "\n",
    "    #print('\\nprediction input looks like:\\n', pred_inp)\n",
    "\n",
    "    # try a prediction:\n",
    "    pred_result = model.predict(pred_inp)\n",
    "\n",
    "    #print('prediction result shape is: ' , np.shape(pred_result))\n",
    "\n",
    "    # reshape prediction:\n",
    "    pred_result = np.reshape(pred_result,(np.shape(pred_result)[1],np.shape(pred_result)[2]))\n",
    "\n",
    "    # need to decode the predictions into a sentence...\n",
    "    pred, conf = decode_prediction(pred_result,temp = 0.1)\n",
    "    #print(pred,conf)\n",
    "\n",
    "    # decode index #'s into real words:\n",
    "    strng = '' # store the word here\n",
    "    for word_idx in pred:\n",
    "        # get the word\n",
    "        strng +=  str(INVLUT[int(word_idx)])\n",
    "        strng += ' ' # add spaces\n",
    "\n",
    "    print('prediction is:\\n',strng)\n",
    "\n",
    "    # continue\n",
    "    choice = input('do another phrase? (y/n): ')\n",
    "    if choice == 'n':\n",
    "        # finish the program!\n",
    "        break\n",
    "        close = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there needs to be some more training happening. This has been left on overnight, however, it is still not capturing language too well. The first few hours training resulted in sentences generated such as  \"a a a a the the a a a the ...\" - so it has come a long way.\n",
    "\n",
    "*I believe that 2  additional things need to happen in order to improve the model:*\n",
    "\n",
    "        1) the data needs to be cleaned up, there is a lot of data that shouldn't be there as a result of my scraping algorithm, such as the targets not being the following verse / not rhyming or being on topic.\n",
    "    \n",
    "        2) the model should have \"attention\" added, which allows the decoding LSTM to look back at inputs rather than just the fixed vector created by the encoding LSTM.\n",
    "        \n",
    "        3) needs MORE data - a few thousand lines just isn't enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('NEW rapML model2.h5') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
